{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "End_to_end_image_compression_stl10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalrdeshmukh/End-to-end-compression/blob/master/End_to_end_image_compression_stl10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SA29G6TcgrZz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Image Compression framework :\n",
        "\n",
        "[End to end image compression framework ](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7999241)"
      ]
    },
    {
      "metadata": {
        "id": "D98I4TsNVy2W",
        "colab_type": "code",
        "outputId": "ce995513-a3c5-464c-f4b8-1bce33d6fc63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn , optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "!pip install Pillow==4.0.0\n",
        "!pip install PIL\n",
        "!pip install image\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.3.0\n",
            "    Uninstalling Pillow-5.3.0:\n",
            "      Successfully uninstalled Pillow-5.3.0\n",
            "Successfully installed Pillow-4.0.0\n",
            "Collecting PIL\n",
            "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (2.1.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.0.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.7)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L9KZr6xwIH9V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "irange = range\n",
        "\n",
        "\n",
        "def make_grid(tensor, nrow=8, padding=2,\n",
        "              normalize=False, range=None, scale_each=False, pad_value=0):\n",
        "    \"\"\"Make a grid of images.\n",
        "    Args:\n",
        "        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
        "            or a list of images all of the same size.\n",
        "        nrow (int, optional): Number of images displayed in each row of the grid.\n",
        "            The Final grid size is (B / nrow, nrow). Default is 8.\n",
        "        padding (int, optional): amount of padding. Default is 2.\n",
        "        normalize (bool, optional): If True, shift the image to the range (0, 1),\n",
        "            by subtracting the minimum and dividing by the maximum pixel value.\n",
        "        range (tuple, optional): tuple (min, max) where min and max are numbers,\n",
        "            then these numbers are used to normalize the image. By default, min and max\n",
        "            are computed from the tensor.\n",
        "        scale_each (bool, optional): If True, scale each image in the batch of\n",
        "            images separately rather than the (min, max) over all images.\n",
        "        pad_value (float, optional): Value for the padded pixels.\n",
        "    Example:\n",
        "        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n",
        "    \"\"\"\n",
        "    if not (torch.is_tensor(tensor) or\n",
        "            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
        "        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n",
        "\n",
        "    # if list of tensors, convert to a 4D mini-batch Tensor\n",
        "    if isinstance(tensor, list):\n",
        "        tensor = torch.stack(tensor, dim=0)\n",
        "\n",
        "    if tensor.dim() == 2:  # single image H x W\n",
        "        tensor = tensor.view(1, tensor.size(0), tensor.size(1))\n",
        "    if tensor.dim() == 3:  # single image\n",
        "        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n",
        "            tensor = torch.cat((tensor, tensor, tensor), 0)\n",
        "        tensor = tensor.view(1, tensor.size(0), tensor.size(1), tensor.size(2))\n",
        "\n",
        "    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n",
        "        tensor = torch.cat((tensor, tensor, tensor), 1)\n",
        "\n",
        "    if normalize is True:\n",
        "        tensor = tensor.clone()  # avoid modifying tensor in-place\n",
        "        if range is not None:\n",
        "            assert isinstance(range, tuple), \\\n",
        "                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n",
        "\n",
        "        def norm_ip(img, min, max):\n",
        "            img.clamp_(min=min, max=max)\n",
        "            img.add_(-min).div_(max - min + 1e-5)\n",
        "\n",
        "        def norm_range(t, range):\n",
        "            if range is not None:\n",
        "                norm_ip(t, range[0], range[1])\n",
        "            else:\n",
        "                norm_ip(t, float(t.min()), float(t.max()))\n",
        "\n",
        "        if scale_each is True:\n",
        "            for t in tensor:  # loop over mini-batch dimension\n",
        "                norm_range(t, range)\n",
        "        else:\n",
        "            norm_range(tensor, range)\n",
        "\n",
        "    if tensor.size(0) == 1:\n",
        "        return tensor.squeeze()\n",
        "\n",
        "    # make the mini-batch of images into a grid\n",
        "    nmaps = tensor.size(0)\n",
        "    xmaps = min(nrow, nmaps)\n",
        "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
        "    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n",
        "    grid = tensor.new(3, height * ymaps + padding, width * xmaps + padding).fill_(pad_value)\n",
        "    k = 0\n",
        "    for y in irange(ymaps):\n",
        "        for x in irange(xmaps):\n",
        "            if k >= nmaps:\n",
        "                break\n",
        "            grid.narrow(1, y * height + padding, height - padding)\\\n",
        "                .narrow(2, x * width + padding, width - padding)\\\n",
        "                .copy_(tensor[k])\n",
        "            k = k + 1\n",
        "    return grid\n",
        "\n",
        "\n",
        "def save_image(tensor, filename, nrow=8, padding=2,\n",
        "               normalize=False, range=None, scale_each=False, pad_value=0):\n",
        "    \"\"\"Save a given Tensor into an image file.\n",
        "    Args:\n",
        "        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n",
        "            saves the tensor as a grid of images by calling ``make_grid``.\n",
        "        **kwargs: Other arguments are documented in ``make_grid``.\n",
        "    \"\"\"\n",
        "    from PIL import Image\n",
        "    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n",
        "                     normalize=normalize, range=range, scale_each=scale_each)\n",
        "    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NG0aiMZcvvcg",
        "colab_type": "code",
        "outputId": "3b8dc472-b5cc-46dc-f5d5-c483121a4b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "trainset = datasets.STL10(root='./data', split='train',download=True, transform=img_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.STL10(root='./data', split='test',download=True, transform=img_transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=16,shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ggg832Bqwbq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# code block to text image loader\n",
        "# print(trainloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KwY8W455pGhk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ]
    },
    {
      "metadata": {
        "id": "G3n2__mWHBQI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CHANNELS = 3\n",
        "HEIGHT = 96\n",
        "WIDTH = 96\n",
        "EPOCHS = 20\n",
        "LOG_INTERVAL = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NOVR5jNOU5A7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the network"
      ]
    },
    {
      "metadata": {
        "id": "U5UBMyfgwO5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Interpolate(nn.Module):\n",
        "    def __init__(self, size, mode):\n",
        "        super(Interpolate, self).__init__()\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.size = size\n",
        "        self.mode = mode\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.interp(x, size=self.size, mode=self.mode, align_corners=False)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "REJrpjnYFeMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class End_to_end(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(End_to_end, self).__init__()\n",
        "    \n",
        "    # Encoder\n",
        "    # TODO : try with padding = 0\n",
        "    self.conv1 = nn.Conv2d(CHANNELS, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0)\n",
        "    self.bn1 = nn.BatchNorm2d(64, affine=False)\n",
        "    self.conv3 = nn.Conv2d(64, CHANNELS, kernel_size=3, stride=1, padding=1)\n",
        "    \n",
        "    # Decoder\n",
        "    #TODO : try ConvTranspose2d\n",
        "    self.interpolate = Interpolate(size=HEIGHT, mode='bilinear')\n",
        "    self.deconv1 = nn.Conv2d(CHANNELS, 64, 3, stride=1, padding=1)\n",
        "    self.deconv2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(64, affine=False)\n",
        "    \n",
        "    self.deconv_n = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "    self.bn_n = nn.BatchNorm2d(64, affine=False)\n",
        "\n",
        "    \n",
        "    self.deconv3 = nn.ConvTranspose2d(64, CHANNELS, 3, stride=1, padding=1)\n",
        "    \n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def encode(self, x):\n",
        "    out = self.relu(self.conv1(x))\n",
        "    out = self.relu(self.conv2(out))\n",
        "    out = self.bn1(out)\n",
        "    return self.conv3(out)\n",
        "    \n",
        "  \n",
        "  def reparameterize(self, mu, logvar):\n",
        "    pass\n",
        "  \n",
        "  def decode(self, z):\n",
        "    upscaled_image = self.interpolate(z)\n",
        "    out = self.relu(self.deconv1(upscaled_image))\n",
        "    out = self.relu(self.deconv2(out))\n",
        "    out = self.bn2(out)\n",
        "    for _ in range(5):\n",
        "      out = self.relu(self.deconv_n(out))\n",
        "      out = self.bn_n(out)\n",
        "    out = self.deconv3(out)\n",
        "    final = upscaled_image + out\n",
        "    return final,out,upscaled_image\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "    com_img = self.encode(x)\n",
        "    final,out,upscaled_image = self.decode(com_img)\n",
        "    return final, out, upscaled_image, com_img, x\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrnHrzxRIP0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "if CUDA:\n",
        "  model = End_to_end().cuda()\n",
        "else :\n",
        "  model = end_to_end()\n",
        "  \n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VfZ9DoJogyOA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_function(final_img,residual_img,upscaled_img,com_img,orig_img):\n",
        "\n",
        "  com_loss = nn.MSELoss(size_average=False)(orig_img, final_img)\n",
        "  rec_loss = nn.MSELoss(size_average=False)(residual_img,orig_img-upscaled_img)\n",
        "  \n",
        "  return com_loss + rec_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWsyut4Kbruf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = Variable(data)\n",
        "        optimizer.zero_grad()\n",
        "        if CUDA:\n",
        "          final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n",
        "        else :\n",
        "          final, residual_img, upscaled_image, com_img, orig_im = model(data)\n",
        "        loss = loss_function(final, residual_img, upscaled_image, com_img, orig_im)\n",
        "        loss.backward()\n",
        "        train_loss += loss.data[0]\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.data[0] / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mpg44nfscmcG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "  \n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  for i, (data, _) in enumerate(test_loader):\n",
        "        data = Variable(data, volatile=True)\n",
        "        if CUDA:\n",
        "          final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n",
        "        else :\n",
        "          final, residual_img, upscaled_image, com_img, orig_im = model(data)\n",
        "        test_loss += loss_function(final, residual_img, upscaled_image, com_img, orig_im).data[0]\n",
        "        if epoch == EPOCHS and i == 0:\n",
        "#             save_image(final.data[0],'reconstruction_final',nrow=8)\n",
        "#             save_image(com_img.data[0],'com_img',nrow=8)\n",
        "            n = min(data.size(0), 6)\n",
        "            print(\"saving the image \"+str(n))\n",
        "            comparison = torch.cat([data[:n],\n",
        "              final[:n].cpu()])\n",
        "\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vwuk-i3jcnIQ",
        "colab_type": "code",
        "outputId": "1b0a6009-a6f7-4b87-cc35-e2fa6cae3b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3267
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/5000 (0%)]\tLoss: 38111.566406\n",
            "Train Epoch: 1 [800/5000 (16%)]\tLoss: 5848.400879\n",
            "Train Epoch: 1 [1600/5000 (32%)]\tLoss: 1606.412842\n",
            "Train Epoch: 1 [2400/5000 (48%)]\tLoss: 756.815430\n",
            "Train Epoch: 1 [3200/5000 (64%)]\tLoss: 565.094238\n",
            "Train Epoch: 1 [4000/5000 (80%)]\tLoss: 462.755920\n",
            "Train Epoch: 1 [4800/5000 (96%)]\tLoss: 612.051575\n",
            "====> Epoch: 1 Average loss: 2709.7944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 497.4062\n",
            "Train Epoch: 2 [0/5000 (0%)]\tLoss: 843.137268\n",
            "Train Epoch: 2 [800/5000 (16%)]\tLoss: 535.899658\n",
            "Train Epoch: 2 [1600/5000 (32%)]\tLoss: 485.820160\n",
            "Train Epoch: 2 [2400/5000 (48%)]\tLoss: 450.467041\n",
            "Train Epoch: 2 [3200/5000 (64%)]\tLoss: 456.669586\n",
            "Train Epoch: 2 [4000/5000 (80%)]\tLoss: 288.428192\n",
            "Train Epoch: 2 [4800/5000 (96%)]\tLoss: 351.939423\n",
            "====> Epoch: 2 Average loss: 428.2243\n",
            "====> Test set loss: 442.4455\n",
            "Train Epoch: 3 [0/5000 (0%)]\tLoss: 365.625946\n",
            "Train Epoch: 3 [800/5000 (16%)]\tLoss: 382.670868\n",
            "Train Epoch: 3 [1600/5000 (32%)]\tLoss: 264.669983\n",
            "Train Epoch: 3 [2400/5000 (48%)]\tLoss: 651.008179\n",
            "Train Epoch: 3 [3200/5000 (64%)]\tLoss: 353.817596\n",
            "Train Epoch: 3 [4000/5000 (80%)]\tLoss: 477.154694\n",
            "Train Epoch: 3 [4800/5000 (96%)]\tLoss: 353.566467\n",
            "====> Epoch: 3 Average loss: 383.3647\n",
            "====> Test set loss: 599.5198\n",
            "Train Epoch: 4 [0/5000 (0%)]\tLoss: 406.398987\n",
            "Train Epoch: 4 [800/5000 (16%)]\tLoss: 401.010864\n",
            "Train Epoch: 4 [1600/5000 (32%)]\tLoss: 381.764343\n",
            "Train Epoch: 4 [2400/5000 (48%)]\tLoss: 347.883514\n",
            "Train Epoch: 4 [3200/5000 (64%)]\tLoss: 360.817596\n",
            "Train Epoch: 4 [4000/5000 (80%)]\tLoss: 300.731750\n",
            "Train Epoch: 4 [4800/5000 (96%)]\tLoss: 387.295288\n",
            "====> Epoch: 4 Average loss: 366.8609\n",
            "====> Test set loss: 323.1987\n",
            "Train Epoch: 5 [0/5000 (0%)]\tLoss: 281.315582\n",
            "Train Epoch: 5 [800/5000 (16%)]\tLoss: 321.437134\n",
            "Train Epoch: 5 [1600/5000 (32%)]\tLoss: 334.055908\n",
            "Train Epoch: 5 [2400/5000 (48%)]\tLoss: 363.366791\n",
            "Train Epoch: 5 [3200/5000 (64%)]\tLoss: 498.080261\n",
            "Train Epoch: 5 [4000/5000 (80%)]\tLoss: 318.990753\n",
            "Train Epoch: 5 [4800/5000 (96%)]\tLoss: 303.958954\n",
            "====> Epoch: 5 Average loss: 388.1638\n",
            "====> Test set loss: 444.3908\n",
            "Train Epoch: 6 [0/5000 (0%)]\tLoss: 436.987640\n",
            "Train Epoch: 6 [800/5000 (16%)]\tLoss: 407.000427\n",
            "Train Epoch: 6 [1600/5000 (32%)]\tLoss: 319.657440\n",
            "Train Epoch: 6 [2400/5000 (48%)]\tLoss: 553.004883\n",
            "Train Epoch: 6 [3200/5000 (64%)]\tLoss: 267.690704\n",
            "Train Epoch: 6 [4000/5000 (80%)]\tLoss: 569.833374\n",
            "Train Epoch: 6 [4800/5000 (96%)]\tLoss: 276.356445\n",
            "====> Epoch: 6 Average loss: 358.8109\n",
            "====> Test set loss: 313.8621\n",
            "Train Epoch: 7 [0/5000 (0%)]\tLoss: 476.037476\n",
            "Train Epoch: 7 [800/5000 (16%)]\tLoss: 427.490814\n",
            "Train Epoch: 7 [1600/5000 (32%)]\tLoss: 327.265045\n",
            "Train Epoch: 7 [2400/5000 (48%)]\tLoss: 366.246094\n",
            "Train Epoch: 7 [3200/5000 (64%)]\tLoss: 373.129333\n",
            "Train Epoch: 7 [4000/5000 (80%)]\tLoss: 316.587799\n",
            "Train Epoch: 7 [4800/5000 (96%)]\tLoss: 234.711060\n",
            "====> Epoch: 7 Average loss: 339.1003\n",
            "====> Test set loss: 473.3465\n",
            "Train Epoch: 8 [0/5000 (0%)]\tLoss: 369.064453\n",
            "Train Epoch: 8 [800/5000 (16%)]\tLoss: 290.453247\n",
            "Train Epoch: 8 [1600/5000 (32%)]\tLoss: 344.609955\n",
            "Train Epoch: 8 [2400/5000 (48%)]\tLoss: 465.986877\n",
            "Train Epoch: 8 [3200/5000 (64%)]\tLoss: 259.675385\n",
            "Train Epoch: 8 [4000/5000 (80%)]\tLoss: 318.437683\n",
            "Train Epoch: 8 [4800/5000 (96%)]\tLoss: 317.010315\n",
            "====> Epoch: 8 Average loss: 336.4783\n",
            "====> Test set loss: 348.7955\n",
            "Train Epoch: 9 [0/5000 (0%)]\tLoss: 275.752594\n",
            "Train Epoch: 9 [800/5000 (16%)]\tLoss: 327.080597\n",
            "Train Epoch: 9 [1600/5000 (32%)]\tLoss: 284.370544\n",
            "Train Epoch: 9 [2400/5000 (48%)]\tLoss: 363.155792\n",
            "Train Epoch: 9 [3200/5000 (64%)]\tLoss: 339.877960\n",
            "Train Epoch: 9 [4000/5000 (80%)]\tLoss: 241.660568\n",
            "Train Epoch: 9 [4800/5000 (96%)]\tLoss: 280.638275\n",
            "====> Epoch: 9 Average loss: 330.6356\n",
            "====> Test set loss: 404.8091\n",
            "Train Epoch: 10 [0/5000 (0%)]\tLoss: 302.826935\n",
            "Train Epoch: 10 [800/5000 (16%)]\tLoss: 283.776550\n",
            "Train Epoch: 10 [1600/5000 (32%)]\tLoss: 177.747742\n",
            "Train Epoch: 10 [2400/5000 (48%)]\tLoss: 286.206421\n",
            "Train Epoch: 10 [3200/5000 (64%)]\tLoss: 231.081177\n",
            "Train Epoch: 10 [4000/5000 (80%)]\tLoss: 256.851135\n",
            "Train Epoch: 10 [4800/5000 (96%)]\tLoss: 252.855209\n",
            "====> Epoch: 10 Average loss: 307.9498\n",
            "====> Test set loss: 334.2177\n",
            "Train Epoch: 11 [0/5000 (0%)]\tLoss: 273.328613\n",
            "Train Epoch: 11 [800/5000 (16%)]\tLoss: 208.043274\n",
            "Train Epoch: 11 [1600/5000 (32%)]\tLoss: 281.478149\n",
            "Train Epoch: 11 [2400/5000 (48%)]\tLoss: 273.287354\n",
            "Train Epoch: 11 [3200/5000 (64%)]\tLoss: 395.839020\n",
            "Train Epoch: 11 [4000/5000 (80%)]\tLoss: 293.254242\n",
            "Train Epoch: 11 [4800/5000 (96%)]\tLoss: 284.100037\n",
            "====> Epoch: 11 Average loss: 309.8658\n",
            "====> Test set loss: 295.2044\n",
            "Train Epoch: 12 [0/5000 (0%)]\tLoss: 372.068665\n",
            "Train Epoch: 12 [800/5000 (16%)]\tLoss: 289.172394\n",
            "Train Epoch: 12 [1600/5000 (32%)]\tLoss: 496.053955\n",
            "Train Epoch: 12 [2400/5000 (48%)]\tLoss: 285.555786\n",
            "Train Epoch: 12 [3200/5000 (64%)]\tLoss: 279.041687\n",
            "Train Epoch: 12 [4000/5000 (80%)]\tLoss: 262.601868\n",
            "Train Epoch: 12 [4800/5000 (96%)]\tLoss: 544.549805\n",
            "====> Epoch: 12 Average loss: 303.8483\n",
            "====> Test set loss: 388.1366\n",
            "Train Epoch: 13 [0/5000 (0%)]\tLoss: 510.536621\n",
            "Train Epoch: 13 [800/5000 (16%)]\tLoss: 314.106934\n",
            "Train Epoch: 13 [1600/5000 (32%)]\tLoss: 297.243164\n",
            "Train Epoch: 13 [2400/5000 (48%)]\tLoss: 296.618195\n",
            "Train Epoch: 13 [3200/5000 (64%)]\tLoss: 240.992828\n",
            "Train Epoch: 13 [4000/5000 (80%)]\tLoss: 404.885162\n",
            "Train Epoch: 13 [4800/5000 (96%)]\tLoss: 322.059479\n",
            "====> Epoch: 13 Average loss: 319.3925\n",
            "====> Test set loss: 504.2306\n",
            "Train Epoch: 14 [0/5000 (0%)]\tLoss: 419.797241\n",
            "Train Epoch: 14 [800/5000 (16%)]\tLoss: 311.870972\n",
            "Train Epoch: 14 [1600/5000 (32%)]\tLoss: 317.966156\n",
            "Train Epoch: 14 [2400/5000 (48%)]\tLoss: 287.337524\n",
            "Train Epoch: 14 [3200/5000 (64%)]\tLoss: 310.925629\n",
            "Train Epoch: 14 [4000/5000 (80%)]\tLoss: 344.700012\n",
            "Train Epoch: 14 [4800/5000 (96%)]\tLoss: 403.800781\n",
            "====> Epoch: 14 Average loss: 309.2458\n",
            "====> Test set loss: 286.6216\n",
            "Train Epoch: 15 [0/5000 (0%)]\tLoss: 240.337219\n",
            "Train Epoch: 15 [800/5000 (16%)]\tLoss: 440.063293\n",
            "Train Epoch: 15 [1600/5000 (32%)]\tLoss: 283.626587\n",
            "Train Epoch: 15 [2400/5000 (48%)]\tLoss: 260.215149\n",
            "Train Epoch: 15 [3200/5000 (64%)]\tLoss: 287.187195\n",
            "Train Epoch: 15 [4000/5000 (80%)]\tLoss: 288.566040\n",
            "Train Epoch: 15 [4800/5000 (96%)]\tLoss: 276.801941\n",
            "====> Epoch: 15 Average loss: 287.9843\n",
            "====> Test set loss: 371.0185\n",
            "Train Epoch: 16 [0/5000 (0%)]\tLoss: 321.191101\n",
            "Train Epoch: 16 [800/5000 (16%)]\tLoss: 266.650024\n",
            "Train Epoch: 16 [1600/5000 (32%)]\tLoss: 254.827942\n",
            "Train Epoch: 16 [2400/5000 (48%)]\tLoss: 242.026138\n",
            "Train Epoch: 16 [3200/5000 (64%)]\tLoss: 199.772751\n",
            "Train Epoch: 16 [4000/5000 (80%)]\tLoss: 187.527908\n",
            "Train Epoch: 16 [4800/5000 (96%)]\tLoss: 250.570404\n",
            "====> Epoch: 16 Average loss: 296.9097\n",
            "====> Test set loss: 308.1890\n",
            "Train Epoch: 17 [0/5000 (0%)]\tLoss: 238.915497\n",
            "Train Epoch: 17 [800/5000 (16%)]\tLoss: 216.668854\n",
            "Train Epoch: 17 [1600/5000 (32%)]\tLoss: 219.927490\n",
            "Train Epoch: 17 [2400/5000 (48%)]\tLoss: 290.272522\n",
            "Train Epoch: 17 [3200/5000 (64%)]\tLoss: 299.387177\n",
            "Train Epoch: 17 [4000/5000 (80%)]\tLoss: 339.004272\n",
            "Train Epoch: 17 [4800/5000 (96%)]\tLoss: 438.502747\n",
            "====> Epoch: 17 Average loss: 311.8845\n",
            "====> Test set loss: 335.2987\n",
            "Train Epoch: 18 [0/5000 (0%)]\tLoss: 259.988708\n",
            "Train Epoch: 18 [800/5000 (16%)]\tLoss: 249.365906\n",
            "Train Epoch: 18 [1600/5000 (32%)]\tLoss: 250.000671\n",
            "Train Epoch: 18 [2400/5000 (48%)]\tLoss: 301.712646\n",
            "Train Epoch: 18 [3200/5000 (64%)]\tLoss: 296.074341\n",
            "Train Epoch: 18 [4000/5000 (80%)]\tLoss: 257.935272\n",
            "Train Epoch: 18 [4800/5000 (96%)]\tLoss: 198.627014\n",
            "====> Epoch: 18 Average loss: 283.3069\n",
            "====> Test set loss: 294.8219\n",
            "Train Epoch: 19 [0/5000 (0%)]\tLoss: 220.949966\n",
            "Train Epoch: 19 [800/5000 (16%)]\tLoss: 326.234528\n",
            "Train Epoch: 19 [1600/5000 (32%)]\tLoss: 308.374939\n",
            "Train Epoch: 19 [2400/5000 (48%)]\tLoss: 315.142944\n",
            "Train Epoch: 19 [3200/5000 (64%)]\tLoss: 324.019104\n",
            "Train Epoch: 19 [4000/5000 (80%)]\tLoss: 291.773315\n",
            "Train Epoch: 19 [4800/5000 (96%)]\tLoss: 295.965637\n",
            "====> Epoch: 19 Average loss: 278.0583\n",
            "====> Test set loss: 309.6525\n",
            "Train Epoch: 20 [0/5000 (0%)]\tLoss: 444.237671\n",
            "Train Epoch: 20 [800/5000 (16%)]\tLoss: 298.154358\n",
            "Train Epoch: 20 [1600/5000 (32%)]\tLoss: 337.792664\n",
            "Train Epoch: 20 [2400/5000 (48%)]\tLoss: 241.214844\n",
            "Train Epoch: 20 [3200/5000 (64%)]\tLoss: 523.580078\n",
            "Train Epoch: 20 [4000/5000 (80%)]\tLoss: 353.949646\n",
            "Train Epoch: 20 [4800/5000 (96%)]\tLoss: 328.949402\n",
            "====> Epoch: 20 Average loss: 294.9229\n",
            "saving the image 6\n",
            "====> Test set loss: 320.8031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IdqFO814nFmd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reference : https://github.com/L1aoXingyu/pytorch-beginner/tree/master/08-AutoEncoder"
      ]
    },
    {
      "metadata": {
        "id": "g0Jjq93jYN3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive') \n",
        "torch.save(model.state_dict(), './net.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "46oJA6aMirJB",
        "colab_type": "code",
        "outputId": "63f7a1ab-f41d-41ba-9582-d38c1e558b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1153
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('net.pth'))\n",
        "\n",
        "def save_images():\n",
        "  epoch = EPOCHS\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  for i, (data, _) in enumerate(test_loader):\n",
        "        data = Variable(data, volatile=True)\n",
        "        if CUDA:\n",
        "          final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n",
        "        else:\n",
        "          final, residual_img, upscaled_image, com_img, orig_im = model(data)\n",
        "\n",
        "        test_loss += loss_function(final, residual_img, upscaled_image, com_img, orig_im).data[0]\n",
        "        if True:\n",
        "#             save_image(final.data[0],'reconstruction_final',nrow=8)\n",
        "#             save_image(com_img.data[0],'com_img',nrow=8)\n",
        "            n = min(data.size(0), 6)\n",
        "            print(\"saving the image \"+str(n))\n",
        "            comparison = torch.cat([data[:n],\n",
        "              final[:n].cpu()])\n",
        "            comparison = comparison.cpu()\n",
        "#             print(comparison.data)\n",
        "            save_image(com_img[i].data,\n",
        "                         'compressed_' + str(i) +'.png', nrow=n)\n",
        "            save_image(final[i].data,\n",
        "                        'final_' + str(i) +'.png', nrow=n)\n",
        "            save_image(orig_im[i].data,\n",
        "                        'original_' + str(i) +'.png', nrow=n)\n",
        "\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "save_images()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n",
            "saving the image 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f930930b470>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
            "    self.worker_result_queue.get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
            "    answer_challenge(c, authkey)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n",
            "    response = connection.recv_bytes(256)        # reject large message\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-54ccb9ec0a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'====> Test set loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0msave_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-54ccb9ec0a6e>\u001b[0m in \u001b[0;36msave_images\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcomparison\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomparison\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#             print(comparison.data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             save_image(com_img[i].data,\n\u001b[0m\u001b[1;32m     25\u001b[0m                          'compressed_' + str(i) +'.png', nrow=n)\n\u001b[1;32m     26\u001b[0m             save_image(final[i].data,\n",
            "\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for dimension 0 with size 16"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "UTFa77gzvLRX",
        "colab_type": "code",
        "outputId": "33fa1056-e883-4ab7-e86e-9391f37c8413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "train(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/5000 (0%)]\tLoss: 254.253082\n",
            "Train Epoch: 0 [800/5000 (16%)]\tLoss: 264.752197\n",
            "Train Epoch: 0 [1600/5000 (32%)]\tLoss: 276.037109\n",
            "Train Epoch: 0 [2400/5000 (48%)]\tLoss: 273.387512\n",
            "Train Epoch: 0 [3200/5000 (64%)]\tLoss: 225.453827\n",
            "Train Epoch: 0 [4000/5000 (80%)]\tLoss: 353.847778\n",
            "Train Epoch: 0 [4800/5000 (96%)]\tLoss: 258.894104\n",
            "====> Epoch: 0 Average loss: 272.8875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZA8DGEHLY-37",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}