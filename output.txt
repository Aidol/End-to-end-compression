2018-09-24 22:38:22.199593: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
iteration: 0
loss 1 :60966652.0
loss 2 :26213.168
iteration: 1
loss 1 :189.93135
loss 2 :30781.875
iteration: 2
loss 1 :1.5320106
loss 2 :23529.898
iteration: 3
loss 1 :0.7510584
loss 2 :25317.312
iteration: 4
loss 1 :0.010353675
loss 2 :23574.719
iteration: 5
loss 1 :0.00088602107
loss 2 :22279.424
iteration: 6
loss 1 :0.0003752887
loss 2 :24151.557
iteration: 7
loss 1 :0.0004194094
loss 2 :25887.201
iteration: 8
loss 1 :0.00045558944
loss 2 :25936.832
iteration: 9
loss 1 :0.0004582421
loss 2 :24183.746
iteration: 10
loss 1 :0.00043244602
loss 2 :22953.703
iteration: 11
loss 1 :0.0003837314
loss 2 :20422.627
iteration: 12
loss 1 :0.00032801265
loss 2 :17624.875
iteration: 13
loss 1 :0.00029163767
loss 2 :15256.715
iteration: 14
loss 1 :0.0002687295
loss 2 :12459.451
iteration: 15
loss 1 :0.00029218657
loss 2 :10342.058
iteration: 16
loss 1 :0.0003528507
loss 2 :9329.765
iteration: 17
loss 1 :0.00043870075
loss 2 :7265.612
iteration: 18
loss 1 :0.00054389454
loss 2 :6096.4062
iteration: 19
loss 1 :0.0006538311
loss 2 :4347.8413
iteration: 20
loss 1 :0.00077069155
loss 2 :3519.6965
iteration: 21
loss 1 :0.0008772598
loss 2 :2713.077
iteration: 22
loss 1 :0.0009315407
loss 2 :2308.0332
iteration: 23
loss 1 :0.0009425352
loss 2 :1857.7181
iteration: 24
loss 1 :0.0009540833
loss 2 :1791.721
iteration: 25
loss 1 :0.0009769847
loss 2 :1457.3934
iteration: 26
loss 1 :0.0009148268
loss 2 :1530.6395
iteration: 27
loss 1 :0.0009138777
loss 2 :1355.5769
iteration: 28
loss 1 :0.0008717935
loss 2 :1268.8195
iteration: 29
loss 1 :0.0008555533
loss 2 :1260.5173
iteration: 30
loss 1 :0.0008183255
loss 2 :1331.6277
iteration: 31
loss 1 :0.00078291126
loss 2 :1274.864
iteration: 32
loss 1 :0.00074111647
loss 2 :1222.7025
iteration: 33
loss 1 :0.00070300227
loss 2 :1191.006
iteration: 34
loss 1 :0.0006865824
loss 2 :1256.2573
iteration: 35
loss 1 :0.0006807929
loss 2 :1139.9927
iteration: 36
loss 1 :0.0006406374
loss 2 :1110.3207
iteration: 37
loss 1 :0.0006645423
loss 2 :1147.4281
iteration: 38
loss 1 :0.00066192163
loss 2 :1139.44
iteration: 39
loss 1 :0.0006965116
loss 2 :1039.4357
iteration: 40
loss 1 :0.00063250546
loss 2 :1058.6387
iteration: 41
loss 1 :0.000784375
loss 2 :989.6398
iteration: 42
loss 1 :0.0007322521
loss 2 :1047.9824
iteration: 43
loss 1 :0.0009732668
loss 2 :1017.2359
iteration: 44
loss 1 :0.0010027204
loss 2 :1113.1611
iteration: 45
loss 1 :0.0010821864
loss 2 :1026.3492
iteration: 46
loss 1 :0.0009257176
loss 2 :1097.4111
iteration: 47
loss 1 :0.000988546
loss 2 :997.02155
iteration: 48
loss 1 :0.0011574217
loss 2 :1025.943
iteration: 49
loss 1 :0.00090153626
loss 2 :1086.2212
iteration: 50
loss 1 :0.0010201402
loss 2 :1102.7677
iteration: 51
loss 1 :0.001076468
loss 2 :1061.9847
iteration: 52
loss 1 :0.0009779395
loss 2 :1049.7528
iteration: 53
loss 1 :0.0010780971
loss 2 :1187.0363
iteration: 54
loss 1 :0.0013027762
loss 2 :1079.8875
iteration: 55
loss 1 :0.0011323782
loss 2 :1018.21356
iteration: 56
loss 1 :0.0015625901
loss 2 :1024.6843
iteration: 57
loss 1 :0.0013842013
loss 2 :1070.345
iteration: 58
loss 1 :0.0015616368
loss 2 :1094.6233
iteration: 59
loss 1 :0.0017085043
loss 2 :1046.27
iteration: 60
loss 1 :0.0018345264
loss 2 :1121.131
iteration: 61
loss 1 :0.0017138945
loss 2 :1114.6295
iteration: 62
loss 1 :0.0021443057
loss 2 :1127.0166
iteration: 63
loss 1 :0.0023504067
loss 2 :1084.1633
iteration: 64
loss 1 :0.0021536995
loss 2 :1080.4961
iteration: 65
loss 1 :0.0024271554
loss 2 :1214.0337
iteration: 66
loss 1 :0.002569574
loss 2 :1176.8376
iteration: 67
loss 1 :0.002362524
loss 2 :1044.2727
iteration: 68
loss 1 :0.0029785084
loss 2 :1163.9276
iteration: 69
loss 1 :0.002730459
loss 2 :1136.0507
iteration: 70
loss 1 :0.0024628812
loss 2 :1133.5461
iteration: 71
loss 1 :0.002634462
loss 2 :1086.8152
iteration: 72
loss 1 :0.0026321851
loss 2 :1080.2255
iteration: 73
loss 1 :0.0024547945
loss 2 :1051.3531
iteration: 74
loss 1 :0.0026616047
loss 2 :1129.3865
iteration: 75
loss 1 :0.0028425956
loss 2 :1157.3477
iteration: 76
loss 1 :0.0026401507
loss 2 :1097.326
iteration: 77
loss 1 :0.0026730567
loss 2 :1173.4048
iteration: 78
loss 1 :0.0029292773
loss 2 :1076.5674
iteration: 79
loss 1 :0.0027217134
loss 2 :1080.4678
iteration: 80
loss 1 :0.002677522
loss 2 :1125.242
iteration: 81
loss 1 :0.0030388522
loss 2 :1102.4374
iteration: 82
loss 1 :0.0028548727
loss 2 :1173.4375
iteration: 83
loss 1 :0.0029612798
loss 2 :1150.8862
iteration: 84
loss 1 :0.0030897858
loss 2 :1091.6887
iteration: 85
loss 1 :0.002641766
loss 2 :1056.9082
iteration: 86
loss 1 :0.003433232
loss 2 :1194.6484
iteration: 87
loss 1 :0.0025852246
loss 2 :1085.7052
iteration: 88
loss 1 :0.0031444586
loss 2 :1088.1565
iteration: 89
loss 1 :0.0029474671
loss 2 :1085.8324
iteration: 90
loss 1 :0.0036319848
loss 2 :1119.3901
iteration: 91
loss 1 :0.0030120288
loss 2 :1129.1086
iteration: 92
loss 1 :0.0034399733
loss 2 :1111.9874
iteration: 93
loss 1 :0.00343672
loss 2 :1098.3716
iteration: 94
loss 1 :0.003344392
loss 2 :1091.2883
iteration: 95
loss 1 :0.003550291
loss 2 :1114.4993
iteration: 96
loss 1 :0.0030598682
loss 2 :1247.0824
iteration: 97
loss 1 :0.003428793
loss 2 :1170.0844
iteration: 98
loss 1 :0.0034104853
loss 2 :1092.174
iteration: 99
loss 1 :0.0031577528
loss 2 :1033.868
iteration: 100
loss 1 :0.0034461147
loss 2 :1046.9429
iteration: 101
loss 1 :0.003342394
loss 2 :1114.0929
iteration: 102
loss 1 :0.0031261551
loss 2 :1127.4446
iteration: 103
loss 1 :0.0033154206
loss 2 :1112.2488
iteration: 104
loss 1 :0.0034558587
loss 2 :1104.5173
iteration: 105
loss 1 :0.0036303117
loss 2 :1134.5107
iteration: 106
loss 1 :0.0031264976
loss 2 :1161.7644
iteration: 107
loss 1 :0.0030468735
loss 2 :1031.1616
iteration: 108
loss 1 :0.0032625191
loss 2 :1068.4316
iteration: 109
loss 1 :0.0030360413
loss 2 :1130.516
iteration: 110
loss 1 :0.0030913642
loss 2 :1039.0531
iteration: 111
loss 1 :0.0030289118
loss 2 :1056.965
iteration: 112
loss 1 :0.0034147876
loss 2 :1129.1237
iteration: 113
loss 1 :0.0033246004
loss 2 :1212.2782
iteration: 114
loss 1 :0.0038001377
loss 2 :1104.248
iteration: 115
loss 1 :0.002813751
loss 2 :1066.7594
iteration: 116
loss 1 :0.003394878
loss 2 :1127.742
iteration: 117
loss 1 :0.0034902247
loss 2 :1020.6362
iteration: 118
loss 1 :0.002952855
loss 2 :1163.745
iteration: 119
loss 1 :0.003509889
loss 2 :1119.0815
iteration: 120
loss 1 :0.0030266251
loss 2 :1085.5222
iteration: 121
loss 1 :0.003208319
loss 2 :1065.4163
iteration: 122
loss 1 :0.0032215337
loss 2 :1099.9589
iteration: 123
loss 1 :0.0033509734
loss 2 :1080.0128
iteration: 124
loss 1 :0.0034512146
loss 2 :1167.5927
iteration: 125
loss 1 :0.0032791414
loss 2 :1074.2539
iteration: 126
loss 1 :0.0034032904
loss 2 :1121.5469
iteration: 127
loss 1 :0.0034496016
loss 2 :1130.2993
iteration: 128
loss 1 :0.0032276982
loss 2 :1141.3801
iteration: 129
loss 1 :0.0034892878
loss 2 :1059.7922
iteration: 130
loss 1 :0.0031596685
loss 2 :1126.2618
iteration: 131
loss 1 :0.0034410183
loss 2 :1076.9576
iteration: 132
loss 1 :0.003692487
loss 2 :1146.3431
iteration: 133
loss 1 :0.002947043
loss 2 :1067.7333
iteration: 134
loss 1 :0.0028280956
loss 2 :1152.387
iteration: 135
loss 1 :0.0031966076
loss 2 :1129.8142
iteration: 136
loss 1 :0.0032103856
loss 2 :1189.3531
iteration: 137
loss 1 :0.0035428952
loss 2 :1154.5999
iteration: 138
loss 1 :0.0030961356
loss 2 :1115.1694
iteration: 139
loss 1 :0.0030813778
loss 2 :1225.8112
iteration: 140
loss 1 :0.0034036567
loss 2 :1129.0221
iteration: 141
loss 1 :0.0035335936
loss 2 :1153.1008
iteration: 142
loss 1 :0.0031083184
loss 2 :1136.4817
iteration: 143
loss 1 :0.003168618
loss 2 :1109.6049
iteration: 144
loss 1 :0.003384851
loss 2 :1105.3456
iteration: 145
loss 1 :0.0031270634
loss 2 :1102.0627
iteration: 146
loss 1 :0.0033158576
loss 2 :1144.8029
iteration: 147
loss 1 :0.0032928782
loss 2 :1022.34735
iteration: 148
loss 1 :0.0032207607
loss 2 :1075.5422
iteration: 149
loss 1 :0.0031887975
loss 2 :1008.9159
iteration: 150
loss 1 :0.0034907465
loss 2 :1025.6184
iteration: 151
loss 1 :0.0035494235
loss 2 :1108.9495
iteration: 152
loss 1 :0.0037591096
loss 2 :1088.8419
iteration: 153
loss 1 :0.0031669778
loss 2 :1119.8167
iteration: 154
loss 1 :0.0033043313
loss 2 :1215.445
iteration: 155
loss 1 :0.0028365497
loss 2 :1104.647
iteration: 156
loss 1 :0.0033126483
loss 2 :1127.084
iteration: 157
loss 1 :0.0029726117
loss 2 :1137.9675
iteration: 158
loss 1 :0.0029483526
loss 2 :1242.3995
iteration: 159
loss 1 :0.0033064426
loss 2 :1132.0051
iteration: 160
loss 1 :0.0031758156
loss 2 :1075.4265
iteration: 161
loss 1 :0.0036006744
loss 2 :1130.895
iteration: 162
loss 1 :0.0037555164
loss 2 :1019.6772
iteration: 163
loss 1 :0.0032806457
loss 2 :1071.6755
iteration: 164
loss 1 :0.0027756137
loss 2 :1056.3256
iteration: 165
loss 1 :0.0031327433
loss 2 :1134.2056
iteration: 166
loss 1 :0.003240631
loss 2 :1173.9014
iteration: 167
loss 1 :0.0030493252
loss 2 :1126.5782
iteration: 168
loss 1 :0.003661659
loss 2 :1119.1093
iteration: 169
loss 1 :0.003480541
loss 2 :1073.7998
iteration: 170
loss 1 :0.0037473422
loss 2 :1105.0457
iteration: 171
loss 1 :0.0029227834
loss 2 :1072.2472
iteration: 172
loss 1 :0.0034494756
loss 2 :1078.6475
iteration: 173
loss 1 :0.0029837778
loss 2 :1139.3845
iteration: 174
loss 1 :0.002823449
loss 2 :1102.2722
iteration: 175
loss 1 :0.003578645
loss 2 :1102.0138
iteration: 176
loss 1 :0.0030138534
loss 2 :1123.0735
iteration: 177
loss 1 :0.003190527
loss 2 :1188.5979
iteration: 178
loss 1 :0.0033661197
loss 2 :1057.5233
iteration: 179
loss 1 :0.0031685063
loss 2 :1204.7261
iteration: 180
loss 1 :0.0034288983
loss 2 :1084.7288
iteration: 181
loss 1 :0.0031252545
loss 2 :1026.3013
iteration: 182
loss 1 :0.0035542645
loss 2 :1106.1516
iteration: 183
loss 1 :0.0036181845
loss 2 :1126.3845
iteration: 184
loss 1 :0.002884757
loss 2 :1095.2805
iteration: 185
loss 1 :0.0032052503
loss 2 :1149.5896
iteration: 186
loss 1 :0.0028756538
loss 2 :1055.0082
iteration: 187
loss 1 :0.003616207
loss 2 :1153.2571
iteration: 188
loss 1 :0.0031841549
loss 2 :1204.2644
iteration: 189
loss 1 :0.0033860216
loss 2 :1159.4484
iteration: 190
loss 1 :0.003463134
loss 2 :1088.1272
iteration: 191
loss 1 :0.0034786938
loss 2 :1082.9473
iteration: 192
loss 1 :0.0033073993
loss 2 :1142.3917
iteration: 193
loss 1 :0.003417387
loss 2 :1116.2952
iteration: 194
loss 1 :0.003184808
loss 2 :1033.7317
iteration: 195
loss 1 :0.003350263
loss 2 :1104.7009
iteration: 196
loss 1 :0.0033459708
loss 2 :1164.4102
iteration: 197
loss 1 :0.003264554
loss 2 :1097.1654
iteration: 198
loss 1 :0.002883282
loss 2 :1173.7985
iteration: 199
loss 1 :0.0034784207
loss 2 :1135.3932
